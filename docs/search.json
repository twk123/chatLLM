[{"path":"https://knowusuboaky.github.io/chatLLM/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Kwadwo Daddy Nyame Owusu Boakye. Author, maintainer.","code":""},{"path":"https://knowusuboaky.github.io/chatLLM/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Owusu Boakye K (2025). chatLLM: Flexible Interface 'LLM' API Interactions. R package version 0.1.2, https://knowusuboaky.github.io/chatLLM/, https://github.com/knowusuboaky/chatLLM.","code":"@Manual{,   title = {chatLLM: A Flexible Interface for 'LLM' API Interactions},   author = {Kwadwo Daddy Nyame {Owusu Boakye}},   year = {2025},   note = {R package version 0.1.2, https://knowusuboaky.github.io/chatLLM/},   url = {https://github.com/knowusuboaky/chatLLM}, }"},{"path":[]},{"path":"https://knowusuboaky.github.io/chatLLM/index.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"A Flexible Interface for LLM API Interactions","text":"chatLLM R package providing single, consistent interface multiple ‚ÄúOpenAI‚Äëcompatible‚Äù chat APIs (OpenAI, Groq, Anthropic, DeepSeek, Alibaba DashScope, GitHub Models). Key features: üîÑ Uniform API across providers üó£ Multi‚Äëmessage context (system/user/assistant roles) üîÅ Retries & backoff clear timeout handling üîà Verbose control (verbose = TRUE/FALSE) ‚öôÔ∏è Discover models via list_models() üèó Factory interface repeated calls üåê Custom endpoint override advanced tuning","code":""},{"path":"https://knowusuboaky.github.io/chatLLM/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"A Flexible Interface for LLM API Interactions","text":"CRAN: Development version:","code":"install.packages(\"chatLLM\") # install.packages(\"remotes\")  # if needed remotes::install_github(\"knowusuboaky/chatLLM\")"},{"path":"https://knowusuboaky.github.io/chatLLM/index.html","id":"setup","dir":"","previous_headings":"","what":"Setup","title":"A Flexible Interface for LLM API Interactions","text":"Set API keys tokens per session:","code":"Sys.setenv(   OPENAI_API_KEY     = \"your-openai-key\",   GROQ_API_KEY       = \"your-groq-key\",   ANTHROPIC_API_KEY  = \"your-anthropic-key\",   DEEPSEEK_API_KEY   = \"your-deepseek-key\",   DASHSCOPE_API_KEY  = \"your-dashscope-key\",   GH_MODELS_TOKEN    = \"your-github-models-token\" )"},{"path":[]},{"path":"https://knowusuboaky.github.io/chatLLM/index.html","id":"id_1-simple-prompt","dir":"","previous_headings":"Usage","what":"1. Simple Prompt","title":"A Flexible Interface for LLM API Interactions","text":"","code":"response <- call_llm(   prompt     = \"Who is Messi?\",   provider   = \"openai\",   max_tokens = 300 ) cat(response)"},{"path":"https://knowusuboaky.github.io/chatLLM/index.html","id":"id_2-multimessage-conversation","dir":"","previous_headings":"Usage","what":"2. Multi‚ÄëMessage Conversation","title":"A Flexible Interface for LLM API Interactions","text":"","code":"conv <- list(   list(role    = \"system\",    content = \"You are a helpful assistant.\"),   list(role    = \"user\",      content = \"Explain recursion in R.\") ) response <- call_llm(   messages          = conv,   provider          = \"openai\",   max_tokens        = 200,   presence_penalty  = 0.2,   frequency_penalty = 0.1,   top_p             = 0.95 ) cat(response)"},{"path":"https://knowusuboaky.github.io/chatLLM/index.html","id":"id_3-verbose-off","dir":"","previous_headings":"Usage","what":"3. Verbose Off","title":"A Flexible Interface for LLM API Interactions","text":"Suppress informational messages:","code":"res <- call_llm(   prompt      = \"Tell me a joke\",   provider    = \"openai\",   verbose     = FALSE ) cat(res)"},{"path":"https://knowusuboaky.github.io/chatLLM/index.html","id":"id_4-factory-interface","dir":"","previous_headings":"Usage","what":"4. Factory Interface","title":"A Flexible Interface for LLM API Interactions","text":"Create reusable LLM function:","code":"# Build a ‚ÄúGitHub Models‚Äù engine with defaults baked in GitHubLLM <- call_llm(   provider    = \"github\",   max_tokens  = 60,   verbose     = FALSE )  # Invoke it like a function: story <- GitHubLLM(\"Tell me a short story about libraries.\") cat(story)"},{"path":"https://knowusuboaky.github.io/chatLLM/index.html","id":"id_5-discover-available-models","dir":"","previous_headings":"Usage","what":"5. Discover Available Models","title":"A Flexible Interface for LLM API Interactions","text":"","code":"# All providers at once all_models <- list_models(\"all\") names(all_models)  # Only OpenAI models openai_models <- list_models(\"openai\") head(openai_models)"},{"path":"https://knowusuboaky.github.io/chatLLM/index.html","id":"id_6-call-a-specific-model","dir":"","previous_headings":"Usage","what":"6. Call a Specific Model","title":"A Flexible Interface for LLM API Interactions","text":"Pick list pass call_llm():","code":"anthro_models <- list_models(\"anthropic\") cat(call_llm(   prompt     = \"Write a haiku about autumn.\",   provider   = \"anthropic\",   model      = anthro_models[1],   max_tokens = 60 ))"},{"path":"https://knowusuboaky.github.io/chatLLM/index.html","id":"troubleshooting","dir":"","previous_headings":"","what":"Troubleshooting","title":"A Flexible Interface for LLM API Interactions","text":"Timeouts: increase n_tries / backoff supply custom .post_func higher timeout(). Model Found: use list_models(\"<provider>\") consult provider docs. Auth Errors: verify API key/token environment variables. Network Issues: check VPN/proxy, firewall, SSL certs.","code":""},{"path":"https://knowusuboaky.github.io/chatLLM/index.html","id":"contributing--support","dir":"","previous_headings":"","what":"Contributing & Support","title":"A Flexible Interface for LLM API Interactions","text":"Issues PRs welcome https://github.com/knowusuboaky/chatLLM","code":""},{"path":"https://knowusuboaky.github.io/chatLLM/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"A Flexible Interface for LLM API Interactions","text":"MIT ¬© Kwadwo Daddy Nyame Owusu - Boakye","code":""},{"path":"https://knowusuboaky.github.io/chatLLM/index.html","id":"acknowledgements","dir":"","previous_headings":"","what":"Acknowledgements","title":"A Flexible Interface for LLM API Interactions","text":"Inspired RAGFlowChainR, powered httr R community. Enjoy!","code":""},{"path":"https://knowusuboaky.github.io/chatLLM/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 Kwadwo Daddy Nyame Owusu Boakye Permission hereby granted, free charge, person obtaining copy software associated documentation files (‚ÄúSoftware‚Äù), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED ‚Äú‚Äù, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://knowusuboaky.github.io/chatLLM/reference/call_llm.html","id":null,"dir":"Reference","previous_headings":"","what":"Unified chat - completion interface ‚Äî call_llm","title":"Unified chat - completion interface ‚Äî call_llm","text":"unified wrapper several \"OpenAI - compatible\" chat - completion APIs (OpenAI, Groq, Anthropic, DeepSeek, Alibaba DashScope, GitHub Models). Accepts either single `prompt` **** full `messages` list, adds correct authentication headers, retries transient failures, returns assistant's text response. can toggle informational console output `verbose = TRUE/FALSE`. chosen `model` longer available, function stops early suggests running `list_models(\"<provider>\")`.","code":""},{"path":"https://knowusuboaky.github.io/chatLLM/reference/call_llm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Unified chat - completion interface ‚Äî call_llm","text":"prompt Character. Single user prompt (optional `messages`). messages List. Full chat history; see *Messages*. provider Character. One `\"openai\"`, `\"groq\"`, `\"anthropic\"`, `\"deepseek\"`, `\"dashscope\"`, `\"github\"`. model Character. Model ID. `NULL`, uses provider default. temperature Numeric. Sampling temperature (0 - 2). Default `0.7`. max_tokens Integer. Max tokens generate. Default `1000`. api_key Character. Override API key; `NULL`, uses environment variable provider. n_tries Integer. Retry attempts failure. Default `3`. backoff Numeric. Seconds retries. Default `2`. verbose Logical. Whether display informational messages (`TRUE`) suppress (`FALSE`). Default `TRUE`. endpoint_url Character. Custom endpoint; `NULL`, sensible provider - specific default used. github_api_version Character. Header `X - GitHub - Api - Version`. Default `\"2022 - 11 - 28\"`. anthropic_api_version Character. Header `anthropic - version`. Default `\"2023 - 06 - 01\"`. ... Extra JSON - body fields (e.g. `top_p`, `stop`, `presence_penalty`). .post_func Internal. HTTP POST function (default `httr::POST`).","code":""},{"path":"https://knowusuboaky.github.io/chatLLM/reference/call_llm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Unified chat - completion interface ‚Äî call_llm","text":"Character scalar: assistant reply text.","code":""},{"path":"https://knowusuboaky.github.io/chatLLM/reference/call_llm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Unified chat - completion interface ‚Äî call_llm","text":"Core chat - completion wrapper multiple providers","code":""},{"path":"https://knowusuboaky.github.io/chatLLM/reference/call_llm.html","id":"messages","dir":"Reference","previous_headings":"","what":"Messages","title":"Unified chat - completion interface ‚Äî call_llm","text":"* `prompt`    -  character scalar treated single *user* message. * `messages`  -  list lists; element must contain `role` `content`.                arguments supplied, `prompt` appended                extra user message.","code":""},{"path":"https://knowusuboaky.github.io/chatLLM/reference/call_llm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Unified chat - completion interface ‚Äî call_llm","text":"","code":"if (FALSE) { # \\dontrun{  ## 1. Listing available models # List all providers at once all_mods <- list_models(\"all\") str(all_mods)  # List OpenAI-only, Groq-only, Anthropic-only openai_mods   <- list_models(\"openai\") groq_mods     <- list_models(\"groq\") anthropic_mods<- list_models(\"anthropic\", anthropic_api_version = \"2023-06-01\")  ## 2. Single-prompt interface  # 2a. Basic usage Sys.setenv(OPENAI_API_KEY = \"sk-...\") res_basic <- call_llm(   prompt   = \"Hello, how are you?\",   provider = \"openai\" ) cat(res_basic)  # 2b. Adjust sampling and penalties res_sampling <- call_llm(   prompt      = \"Write a haiku about winter\",   provider    = \"openai\",   temperature = 1.2,   top_p       = 0.5,   presence_penalty  = 0.6,   frequency_penalty = 0.4 ) cat(res_sampling)  # 2c. Control length and retries res_len <- call_llm(   prompt      = \"List 5 uses for R\",   provider    = \"openai\",   max_tokens  = 50,   n_tries     = 5,   backoff     = 0.5 ) cat(res_len)  # 2d. Using stop sequences res_stop <- call_llm(   prompt   = \"Count from 1 to 10:\",   provider = \"openai\",   stop     = c(\"6\") ) cat(res_stop)  # 2e. Override API key for one call res_override <- call_llm(   prompt   = \"Override test\",   provider = \"openai\",   api_key  = \"sk-override\",   max_tokens = 20 ) cat(res_override)  # 2f. Factory interface for repeated prompts GitHubLLM <- call_llm(   provider   = \"github\",   max_tokens = 60,   verbose    = FALSE ) # direct invocation story1 <- GitHubLLM(\"Tell me a short story\") cat(story1)  ## 3. Multi-message conversation  # 3a. Simple system + user convo1 <- list(   list(role = \"system\",    content = \"You are a helpful assistant.\"),   list(role = \"user\",      content = \"Explain recursion.\") ) res1 <- call_llm(   messages   = convo1,   provider   = \"openai\",   max_tokens = 100 ) cat(res1)  # 3b. Continue an existing chat by appending a prompt prev <- list(   list(role = \"system\", content = \"You are concise.\"),   list(role = \"user\",   content = \"Summarize the plot of Hamlet.\") ) res2 <- call_llm(   messages = prev,   prompt   = \"Now give me three bullet points.\" ) cat(res2)  # 3c. Use stop sequence in multi-message convo2 <- list(   list(role = \"system\", content = \"You list items.\"),   list(role = \"user\",   content = \"Name three colors.\") ) res3 <- call_llm(   messages = convo2,   stop     = c(\".\") ) cat(res3)  # 3d. Multi-message via factory interface ScopedLLM <- call_llm(provider = \"openai\", temperature = 0.3) chat_ctx <- list(   list(role = \"system\", content = \"You are a math tutor.\") ) ans1 <- ScopedLLM(messages = chat_ctx, prompt = \"Solve 2+2.\") cat(ans1) ans2 <- ScopedLLM(\"What about 10*10?\") cat(ans2) } # }"},{"path":"https://knowusuboaky.github.io/chatLLM/reference/list_models.html","id":null,"dir":"Reference","previous_headings":"","what":"List Available Models for Supported Providers ‚Äî list_models","title":"List Available Models for Supported Providers ‚Äî list_models","text":"Retrieve catalog available model IDs one supported chat - completion providers. Useful discovering active models avoiding typos deprecated defaults. Supported providers: \"openai\"     -  OpenAI Chat Completions API \"groq\"       -  Groq OpenAI - compatible endpoint \"anthropic\"  -  Anthropic Claude API \"deepseek\"   -  DeepSeek chat API \"dashscope\"  -  Alibaba DashScope compatible API \"github\"     -  GitHub Models OpenAI - compatible API \"\"        -  Fetch catalogs ","code":""},{"path":"https://knowusuboaky.github.io/chatLLM/reference/list_models.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Available Models for Supported Providers ‚Äî list_models","text":"provider Character. One \"github\", \"openai\", \"groq\", \"anthropic\", \"deepseek\", \"dashscope\" \"\". Case - insensitive. ... Additional arguments passed per - provider helper (e.g. limit Anthropic, api_version GitHub). github_api_version Character. Header value X - GitHub - Api - Version (GitHub Models). Default \"2022 - 11 - 28\". anthropic_api_version Character. Header value anthropic - version (Anthropic). Default \"2023 - 06 - 01\".","code":""},{"path":"https://knowusuboaky.github.io/chatLLM/reference/list_models.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List Available Models for Supported Providers ‚Äî list_models","text":"provider != \"\", character vector model IDs single provider. provider == \"\", named list character vectors, one per provider.","code":""},{"path":[]},{"path":"https://knowusuboaky.github.io/chatLLM/reference/list_models.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"List Available Models for Supported Providers ‚Äî list_models","text":"","code":"if (FALSE) { # \\dontrun{ Sys.setenv(OPENAI_API_KEY = \"sk-...\") openai_models <- list_models(\"openai\") head(openai_models)  Sys.setenv(ANTHROPIC_API_KEY = \"sk-...\") anthro_models <- list_models(\"anthropic\", anthropic_api_version = \"2023-06-01\")  Sys.setenv(GH_MODELS_TOKEN = \"ghp-...\") github_models <- list_models(\"github\", github_api_version = \"2022-11-28\") } # }"},{"path":[]},{"path":"https://knowusuboaky.github.io/chatLLM/news/index.html","id":"new-features-0-1-2","dir":"Changelog","previous_headings":"","what":"New Features","title":"chatLLM 0.1.2 (Upcoming Release ‚Äì May 2025)","text":"DeepSeek Integrationchat_llm() now supports DeepSeek backend provider. expands range available language models increases flexibility users selecting different inference engines. Alibaba DashScope Integration can now use models Alibaba Cloud‚Äôs Model Studio (DashScope) via OpenAI-compatible endpoints. allows users mainland China beyond easily integrate powerful Qwen-series models (like qwen-plus, qwen-turbo, others) using chat_llm() interface. GitHub Copilot-Compatible Model Integration can now use models hosted GitHub Copilot-compatible endpoints. allows seamless integration custom-hosted proxy-accessible models, making easier experiment private specialized deployments. Model Catalog Accesschat_llm() now supports listing available models across supported providers. makes easier discover compare model options selecting one workflow.","code":""}]
