<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Unified chat - completion interface — call_llm • chatLLM</title><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"><link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"><script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Unified chat - completion interface — call_llm"><meta name="description" content="A unified wrapper for several &quot;OpenAI - compatible&quot; chat - completion APIs
(OpenAI, Groq, Anthropic, DeepSeek, Alibaba DashScope, GitHub Models).
Accepts either a single `prompt` **or** a full `messages` list, adds the
correct authentication headers, retries on transient failures, and returns
the assistant's text response. You can toggle informational console
output with `verbose = TRUE/FALSE`. If the chosen `model` is no longer
available, the function stops early and suggests running
`list_models(&quot;&amp;lt;provider&amp;gt;&quot;)`."><meta property="og:description" content="A unified wrapper for several &quot;OpenAI - compatible&quot; chat - completion APIs
(OpenAI, Groq, Anthropic, DeepSeek, Alibaba DashScope, GitHub Models).
Accepts either a single `prompt` **or** a full `messages` list, adds the
correct authentication headers, retries on transient failures, and returns
the assistant's text response. You can toggle informational console
output with `verbose = TRUE/FALSE`. If the chosen `model` is no longer
available, the function stops early and suggests running
`list_models(&quot;&amp;lt;provider&amp;gt;&quot;)`."></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">chatLLM</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.2</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="active nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json"></form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/knowusuboaky/chatLLM/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul></div>


  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Unified chat - completion interface</h1>
      <small class="dont-index">Source: <a href="https://github.com/knowusuboaky/chatLLM/blob/main/R/chatLLM.R" class="external-link"><code>R/chatLLM.R</code></a></small>
      <div class="d-none name"><code>call_llm.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p>A unified wrapper for several "OpenAI - compatible" chat - completion APIs
(OpenAI, Groq, Anthropic, DeepSeek, Alibaba DashScope, GitHub Models).
Accepts either a single `prompt` **or** a full `messages` list, adds the
correct authentication headers, retries on transient failures, and returns
the assistant's text response. You can toggle informational console
output with `verbose = TRUE/FALSE`. If the chosen `model` is no longer
available, the function stops early and suggests running
`list_models("&lt;provider&gt;")`.</p>
    </div>


    <div class="section level2">
    <h2 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h2>


<dl><dt id="arg-prompt">prompt<a class="anchor" aria-label="anchor" href="#arg-prompt"></a></dt>
<dd><p>Character. Single user prompt (optional if `messages`).</p></dd>


<dt id="arg-messages">messages<a class="anchor" aria-label="anchor" href="#arg-messages"></a></dt>
<dd><p>List. Full chat history; see *Messages*.</p></dd>


<dt id="arg-provider">provider<a class="anchor" aria-label="anchor" href="#arg-provider"></a></dt>
<dd><p>Character. One of `"openai"`, `"groq"`, `"anthropic"`,
`"deepseek"`, `"dashscope"`, or `"github"`.</p></dd>


<dt id="arg-model">model<a class="anchor" aria-label="anchor" href="#arg-model"></a></dt>
<dd><p>Character. Model ID. If `NULL`, uses the provider default.</p></dd>


<dt id="arg-temperature">temperature<a class="anchor" aria-label="anchor" href="#arg-temperature"></a></dt>
<dd><p>Numeric. Sampling temperature (0 - 2). Default `0.7`.</p></dd>


<dt id="arg-max-tokens">max_tokens<a class="anchor" aria-label="anchor" href="#arg-max-tokens"></a></dt>
<dd><p>Integer. Max tokens to generate. Default `1000`.</p></dd>


<dt id="arg-api-key">api_key<a class="anchor" aria-label="anchor" href="#arg-api-key"></a></dt>
<dd><p>Character. Override API key; if `NULL`, uses the
environment variable for that provider.</p></dd>


<dt id="arg-n-tries">n_tries<a class="anchor" aria-label="anchor" href="#arg-n-tries"></a></dt>
<dd><p>Integer. Retry attempts on failure. Default `3`.</p></dd>


<dt id="arg-backoff">backoff<a class="anchor" aria-label="anchor" href="#arg-backoff"></a></dt>
<dd><p>Numeric. Seconds between retries. Default `2`.</p></dd>


<dt id="arg-verbose">verbose<a class="anchor" aria-label="anchor" href="#arg-verbose"></a></dt>
<dd><p>Logical. Whether to display informational messages
(`TRUE`) or suppress them (`FALSE`). Default `TRUE`.</p></dd>


<dt id="arg-endpoint-url">endpoint_url<a class="anchor" aria-label="anchor" href="#arg-endpoint-url"></a></dt>
<dd><p>Character. Custom endpoint; if `NULL`, a sensible
provider - specific default is used.</p></dd>


<dt id="arg-github-api-version">github_api_version<a class="anchor" aria-label="anchor" href="#arg-github-api-version"></a></dt>
<dd><p>Character. Header `X - GitHub - Api - Version`.
Default `"2022 - 11 - 28"`.</p></dd>


<dt id="arg-anthropic-api-version">anthropic_api_version<a class="anchor" aria-label="anchor" href="#arg-anthropic-api-version"></a></dt>
<dd><p>Character. Header `anthropic - version`.
Default `"2023 - 06 - 01"`.</p></dd>


<dt id="arg--">...<a class="anchor" aria-label="anchor" href="#arg--"></a></dt>
<dd><p>Extra JSON - body fields (e.g. `top_p`, `stop`,
`presence_penalty`).</p></dd>


<dt id="arg--post-func">.post_func<a class="anchor" aria-label="anchor" href="#arg--post-func"></a></dt>
<dd><p>Internal. HTTP POST function (default `httr::POST`).</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="value">Value<a class="anchor" aria-label="anchor" href="#value"></a></h2>
    <p>Character scalar: assistant reply text.</p>
    </div>
    <div class="section level2">
    <h2 id="details">Details<a class="anchor" aria-label="anchor" href="#details"></a></h2>
    <p>Core chat - completion wrapper for multiple providers</p>
    </div>
    <div class="section level2">
    <h2 id="messages">Messages<a class="anchor" aria-label="anchor" href="#messages"></a></h2>


<p>* `prompt`    -  character scalar treated as a single *user* message.
* `messages`  -  list of lists; each element must contain `role` and `content`.
               If both arguments are supplied, the `prompt` is appended
               as an extra user message.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-examples">Examples<a class="anchor" aria-label="anchor" href="#ref-examples"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span class="r-in"><span><span class="kw">if</span> <span class="op">(</span><span class="cn">FALSE</span><span class="op">)</span> <span class="op">{</span> <span class="co"># \dontrun{</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## 1. Listing available models</span></span></span>
<span class="r-in"><span><span class="co"># List all providers at once</span></span></span>
<span class="r-in"><span><span class="va">all_mods</span> <span class="op">&lt;-</span> <span class="fu"><a href="list_models.html">list_models</a></span><span class="op">(</span><span class="st">"all"</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/utils/str.html" class="external-link">str</a></span><span class="op">(</span><span class="va">all_mods</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># List OpenAI-only, Groq-only, Anthropic-only</span></span></span>
<span class="r-in"><span><span class="va">openai_mods</span>   <span class="op">&lt;-</span> <span class="fu"><a href="list_models.html">list_models</a></span><span class="op">(</span><span class="st">"openai"</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">groq_mods</span>     <span class="op">&lt;-</span> <span class="fu"><a href="list_models.html">list_models</a></span><span class="op">(</span><span class="st">"groq"</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">anthropic_mods</span><span class="op">&lt;-</span> <span class="fu"><a href="list_models.html">list_models</a></span><span class="op">(</span><span class="st">"anthropic"</span>, anthropic_api_version <span class="op">=</span> <span class="st">"2023-06-01"</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## 2. Single-prompt interface</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># 2a. Basic usage</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/Sys.setenv.html" class="external-link">Sys.setenv</a></span><span class="op">(</span>OPENAI_API_KEY <span class="op">=</span> <span class="st">"sk-..."</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">res_basic</span> <span class="op">&lt;-</span> <span class="fu">call_llm</span><span class="op">(</span></span></span>
<span class="r-in"><span>  prompt   <span class="op">=</span> <span class="st">"Hello, how are you?"</span>,</span></span>
<span class="r-in"><span>  provider <span class="op">=</span> <span class="st">"openai"</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="va">res_basic</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># 2b. Adjust sampling and penalties</span></span></span>
<span class="r-in"><span><span class="va">res_sampling</span> <span class="op">&lt;-</span> <span class="fu">call_llm</span><span class="op">(</span></span></span>
<span class="r-in"><span>  prompt      <span class="op">=</span> <span class="st">"Write a haiku about winter"</span>,</span></span>
<span class="r-in"><span>  provider    <span class="op">=</span> <span class="st">"openai"</span>,</span></span>
<span class="r-in"><span>  temperature <span class="op">=</span> <span class="fl">1.2</span>,</span></span>
<span class="r-in"><span>  top_p       <span class="op">=</span> <span class="fl">0.5</span>,</span></span>
<span class="r-in"><span>  presence_penalty  <span class="op">=</span> <span class="fl">0.6</span>,</span></span>
<span class="r-in"><span>  frequency_penalty <span class="op">=</span> <span class="fl">0.4</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="va">res_sampling</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># 2c. Control length and retries</span></span></span>
<span class="r-in"><span><span class="va">res_len</span> <span class="op">&lt;-</span> <span class="fu">call_llm</span><span class="op">(</span></span></span>
<span class="r-in"><span>  prompt      <span class="op">=</span> <span class="st">"List 5 uses for R"</span>,</span></span>
<span class="r-in"><span>  provider    <span class="op">=</span> <span class="st">"openai"</span>,</span></span>
<span class="r-in"><span>  max_tokens  <span class="op">=</span> <span class="fl">50</span>,</span></span>
<span class="r-in"><span>  n_tries     <span class="op">=</span> <span class="fl">5</span>,</span></span>
<span class="r-in"><span>  backoff     <span class="op">=</span> <span class="fl">0.5</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="va">res_len</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># 2d. Using stop sequences</span></span></span>
<span class="r-in"><span><span class="va">res_stop</span> <span class="op">&lt;-</span> <span class="fu">call_llm</span><span class="op">(</span></span></span>
<span class="r-in"><span>  prompt   <span class="op">=</span> <span class="st">"Count from 1 to 10:"</span>,</span></span>
<span class="r-in"><span>  provider <span class="op">=</span> <span class="st">"openai"</span>,</span></span>
<span class="r-in"><span>  stop     <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"6"</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="va">res_stop</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># 2e. Override API key for one call</span></span></span>
<span class="r-in"><span><span class="va">res_override</span> <span class="op">&lt;-</span> <span class="fu">call_llm</span><span class="op">(</span></span></span>
<span class="r-in"><span>  prompt   <span class="op">=</span> <span class="st">"Override test"</span>,</span></span>
<span class="r-in"><span>  provider <span class="op">=</span> <span class="st">"openai"</span>,</span></span>
<span class="r-in"><span>  api_key  <span class="op">=</span> <span class="st">"sk-override"</span>,</span></span>
<span class="r-in"><span>  max_tokens <span class="op">=</span> <span class="fl">20</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="va">res_override</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># 2f. Factory interface for repeated prompts</span></span></span>
<span class="r-in"><span><span class="va">GitHubLLM</span> <span class="op">&lt;-</span> <span class="fu">call_llm</span><span class="op">(</span></span></span>
<span class="r-in"><span>  provider   <span class="op">=</span> <span class="st">"github"</span>,</span></span>
<span class="r-in"><span>  max_tokens <span class="op">=</span> <span class="fl">60</span>,</span></span>
<span class="r-in"><span>  verbose    <span class="op">=</span> <span class="cn">FALSE</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="co"># direct invocation</span></span></span>
<span class="r-in"><span><span class="va">story1</span> <span class="op">&lt;-</span> <span class="fu">GitHubLLM</span><span class="op">(</span><span class="st">"Tell me a short story"</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="va">story1</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co">## 3. Multi-message conversation</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># 3a. Simple system + user</span></span></span>
<span class="r-in"><span><span class="va">convo1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span></span>
<span class="r-in"><span>  <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>role <span class="op">=</span> <span class="st">"system"</span>,    content <span class="op">=</span> <span class="st">"You are a helpful assistant."</span><span class="op">)</span>,</span></span>
<span class="r-in"><span>  <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>role <span class="op">=</span> <span class="st">"user"</span>,      content <span class="op">=</span> <span class="st">"Explain recursion."</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">res1</span> <span class="op">&lt;-</span> <span class="fu">call_llm</span><span class="op">(</span></span></span>
<span class="r-in"><span>  messages   <span class="op">=</span> <span class="va">convo1</span>,</span></span>
<span class="r-in"><span>  provider   <span class="op">=</span> <span class="st">"openai"</span>,</span></span>
<span class="r-in"><span>  max_tokens <span class="op">=</span> <span class="fl">100</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="va">res1</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># 3b. Continue an existing chat by appending a prompt</span></span></span>
<span class="r-in"><span><span class="va">prev</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span></span>
<span class="r-in"><span>  <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>role <span class="op">=</span> <span class="st">"system"</span>, content <span class="op">=</span> <span class="st">"You are concise."</span><span class="op">)</span>,</span></span>
<span class="r-in"><span>  <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>role <span class="op">=</span> <span class="st">"user"</span>,   content <span class="op">=</span> <span class="st">"Summarize the plot of Hamlet."</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">res2</span> <span class="op">&lt;-</span> <span class="fu">call_llm</span><span class="op">(</span></span></span>
<span class="r-in"><span>  messages <span class="op">=</span> <span class="va">prev</span>,</span></span>
<span class="r-in"><span>  prompt   <span class="op">=</span> <span class="st">"Now give me three bullet points."</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="va">res2</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># 3c. Use stop sequence in multi-message</span></span></span>
<span class="r-in"><span><span class="va">convo2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span></span>
<span class="r-in"><span>  <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>role <span class="op">=</span> <span class="st">"system"</span>, content <span class="op">=</span> <span class="st">"You list items."</span><span class="op">)</span>,</span></span>
<span class="r-in"><span>  <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>role <span class="op">=</span> <span class="st">"user"</span>,   content <span class="op">=</span> <span class="st">"Name three colors."</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">res3</span> <span class="op">&lt;-</span> <span class="fu">call_llm</span><span class="op">(</span></span></span>
<span class="r-in"><span>  messages <span class="op">=</span> <span class="va">convo2</span>,</span></span>
<span class="r-in"><span>  stop     <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"."</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="va">res3</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># 3d. Multi-message via factory interface</span></span></span>
<span class="r-in"><span><span class="va">ScopedLLM</span> <span class="op">&lt;-</span> <span class="fu">call_llm</span><span class="op">(</span>provider <span class="op">=</span> <span class="st">"openai"</span>, temperature <span class="op">=</span> <span class="fl">0.3</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">chat_ctx</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span></span>
<span class="r-in"><span>  <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>role <span class="op">=</span> <span class="st">"system"</span>, content <span class="op">=</span> <span class="st">"You are a math tutor."</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">ans1</span> <span class="op">&lt;-</span> <span class="fu">ScopedLLM</span><span class="op">(</span>messages <span class="op">=</span> <span class="va">chat_ctx</span>, prompt <span class="op">=</span> <span class="st">"Solve 2+2."</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="va">ans1</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">ans2</span> <span class="op">&lt;-</span> <span class="fu">ScopedLLM</span><span class="op">(</span><span class="st">"What about 10*10?"</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="va">ans2</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">}</span> <span class="co"># }</span></span></span>
<span class="r-in"><span></span></span>
</code></pre></div>
    </div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Kwadwo Daddy Nyame Owusu Boakye.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.2.</p>
</div>

    </footer></div>





  </body></html>

