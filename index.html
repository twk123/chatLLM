<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>A Flexible Interface for LLM API Interactions ‚Ä¢ chatLLM</title>
<script src="deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="deps/headroom-0.11.0/headroom.min.js"></script><script src="deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="deps/search-1.0.0/fuse.min.js"></script><script src="deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="pkgdown.js"></script><meta property="og:title" content="A Flexible Interface for LLM API Interactions">
<meta name="description" content="Provides a flexible interface for interacting with Large Language Model (LLM) providers including OpenAI, Groq, Anthropic, DeepSeek, DashScope, and GitHub Models. Supports both synchronous and asynchronous chat-completion APIs, with features such as retry logic, dynamic model selection, customizable parameters, and multi-message conversation handling. Designed to streamline integration with state-of-the-art LLM services across multiple platforms.">
<meta property="og:description" content="Provides a flexible interface for interacting with Large Language Model (LLM) providers including OpenAI, Groq, Anthropic, DeepSeek, DashScope, and GitHub Models. Supports both synchronous and asynchronous chat-completion APIs, with features such as retry logic, dynamic model selection, customizable parameters, and multi-message conversation handling. Designed to streamline integration with state-of-the-art LLM services across multiple platforms.">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="index.html">chatLLM</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.2</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="reference/index.html">Reference</a></li>
<li class="nav-item"><a class="nav-link" href="news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/knowusuboaky/chatLLM/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-home">
<div class="row">
  <main id="main" class="col-md-9"><div class="section level1">
<div class="page-header"><h1 id="chatllm-">chatLLM <a href="https://knowusuboaky.github.io/chatLLM/"><img src="reference/figures/openlogo.png" align="right" height="120"></a>
<a class="anchor" aria-label="anchor" href="#chatllm-"></a>
</h1></div>
<!-- badges: start -->

<!-- badges: end -->
<div class="section level2">
<h2 id="overview">Overview<a class="anchor" aria-label="anchor" href="#overview"></a>
</h2>
<p><strong>chatLLM</strong> is an R package providing a single, consistent interface to multiple ‚ÄúOpenAI‚Äëcompatible‚Äù chat APIs (OpenAI, Groq, Anthropic, DeepSeek, Alibaba DashScope, and GitHub Models).</p>
<p>Key features:</p>
<ul>
<li>üîÑ <strong>Uniform API</strong> across providers</li>
<li>üó£ <strong>Multi‚Äëmessage context</strong> (system/user/assistant roles)</li>
<li>üîÅ <strong>Retries &amp; backoff</strong> with clear timeout handling</li>
<li>üîà <strong>Verbose control</strong> (<code>verbose = TRUE/FALSE</code>)</li>
<li>‚öôÔ∏è <strong>Discover models</strong> via <code><a href="reference/list_models.html">list_models()</a></code>
</li>
<li>üèó <strong>Factory interface</strong> for repeated calls</li>
<li>üåê <strong>Custom endpoint</strong> override and advanced tuning</li>
</ul>
<hr>
</div>
<div class="section level2">
<h2 id="installation">Installation<a class="anchor" aria-label="anchor" href="#installation"></a>
</h2>
<p>From CRAN:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html" class="external-link">install.packages</a></span><span class="op">(</span><span class="st">"chatLLM"</span><span class="op">)</span></span></code></pre></div>
<p>Development version:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># install.packages("remotes")  # if needed</span></span>
<span><span class="fu">remotes</span><span class="fu">::</span><span class="fu">install_github</span><span class="op">(</span><span class="st">"knowusuboaky/chatLLM"</span><span class="op">)</span></span></code></pre></div>
<hr>
</div>
<div class="section level2">
<h2 id="setup">Setup<a class="anchor" aria-label="anchor" href="#setup"></a>
</h2>
<p>Set your API keys or tokens once per session:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Sys.setenv.html" class="external-link">Sys.setenv</a></span><span class="op">(</span></span>
<span>  OPENAI_API_KEY     <span class="op">=</span> <span class="st">"your-openai-key"</span>,</span>
<span>  GROQ_API_KEY       <span class="op">=</span> <span class="st">"your-groq-key"</span>,</span>
<span>  ANTHROPIC_API_KEY  <span class="op">=</span> <span class="st">"your-anthropic-key"</span>,</span>
<span>  DEEPSEEK_API_KEY   <span class="op">=</span> <span class="st">"your-deepseek-key"</span>,</span>
<span>  DASHSCOPE_API_KEY  <span class="op">=</span> <span class="st">"your-dashscope-key"</span>,</span>
<span>  GH_MODELS_TOKEN    <span class="op">=</span> <span class="st">"your-github-models-token"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<hr>
</div>
<div class="section level2">
<h2 id="usage">Usage<a class="anchor" aria-label="anchor" href="#usage"></a>
</h2>
<div class="section level3">
<h3 id="id_1-simple-prompt">1. Simple Prompt<a class="anchor" aria-label="anchor" href="#id_1-simple-prompt"></a>
</h3>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">response</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/call_llm.html">call_llm</a></span><span class="op">(</span></span>
<span>  prompt     <span class="op">=</span> <span class="st">"Who is Messi?"</span>,</span>
<span>  provider   <span class="op">=</span> <span class="st">"openai"</span>,</span>
<span>  max_tokens <span class="op">=</span> <span class="fl">300</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="va">response</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="id_2-multimessage-conversation">2. Multi‚ÄëMessage Conversation<a class="anchor" aria-label="anchor" href="#id_2-multimessage-conversation"></a>
</h3>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">conv</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>role    <span class="op">=</span> <span class="st">"system"</span>,    content <span class="op">=</span> <span class="st">"You are a helpful assistant."</span><span class="op">)</span>,</span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>role    <span class="op">=</span> <span class="st">"user"</span>,      content <span class="op">=</span> <span class="st">"Explain recursion in R."</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="va">response</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/call_llm.html">call_llm</a></span><span class="op">(</span></span>
<span>  messages          <span class="op">=</span> <span class="va">conv</span>,</span>
<span>  provider          <span class="op">=</span> <span class="st">"openai"</span>,</span>
<span>  max_tokens        <span class="op">=</span> <span class="fl">200</span>,</span>
<span>  presence_penalty  <span class="op">=</span> <span class="fl">0.2</span>,</span>
<span>  frequency_penalty <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span>  top_p             <span class="op">=</span> <span class="fl">0.95</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="va">response</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="id_3-verbose-off">3. Verbose Off<a class="anchor" aria-label="anchor" href="#id_3-verbose-off"></a>
</h3>
<p>Suppress informational messages:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">res</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/call_llm.html">call_llm</a></span><span class="op">(</span></span>
<span>  prompt      <span class="op">=</span> <span class="st">"Tell me a joke"</span>,</span>
<span>  provider    <span class="op">=</span> <span class="st">"openai"</span>,</span>
<span>  verbose     <span class="op">=</span> <span class="cn">FALSE</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="va">res</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="id_4-factory-interface">4. Factory Interface<a class="anchor" aria-label="anchor" href="#id_4-factory-interface"></a>
</h3>
<p>Create a reusable LLM function:</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Build a ‚ÄúGitHub Models‚Äù engine with defaults baked in</span></span>
<span><span class="va">GitHubLLM</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/call_llm.html">call_llm</a></span><span class="op">(</span></span>
<span>  provider    <span class="op">=</span> <span class="st">"github"</span>,</span>
<span>  max_tokens  <span class="op">=</span> <span class="fl">60</span>,</span>
<span>  verbose     <span class="op">=</span> <span class="cn">FALSE</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Invoke it like a function:</span></span>
<span><span class="va">story</span> <span class="op">&lt;-</span> <span class="fu">GitHubLLM</span><span class="op">(</span><span class="st">"Tell me a short story about libraries."</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="va">story</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="id_5-discover-available-models">5. Discover Available Models<a class="anchor" aria-label="anchor" href="#id_5-discover-available-models"></a>
</h3>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># All providers at once</span></span>
<span><span class="va">all_models</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/list_models.html">list_models</a></span><span class="op">(</span><span class="st">"all"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/names.html" class="external-link">names</a></span><span class="op">(</span><span class="va">all_models</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Only OpenAI models</span></span>
<span><span class="va">openai_models</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/list_models.html">list_models</a></span><span class="op">(</span><span class="st">"openai"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">openai_models</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="id_6-call-a-specific-model">6. Call a Specific Model<a class="anchor" aria-label="anchor" href="#id_6-call-a-specific-model"></a>
</h3>
<p>Pick from the list and pass it to <code><a href="reference/call_llm.html">call_llm()</a></code>:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">anthro_models</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/list_models.html">list_models</a></span><span class="op">(</span><span class="st">"anthropic"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="fu"><a href="reference/call_llm.html">call_llm</a></span><span class="op">(</span></span>
<span>  prompt     <span class="op">=</span> <span class="st">"Write a haiku about autumn."</span>,</span>
<span>  provider   <span class="op">=</span> <span class="st">"anthropic"</span>,</span>
<span>  model      <span class="op">=</span> <span class="va">anthro_models</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>,</span>
<span>  max_tokens <span class="op">=</span> <span class="fl">60</span></span>
<span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<hr>
</div>
</div>
<div class="section level2">
<h2 id="troubleshooting">Troubleshooting<a class="anchor" aria-label="anchor" href="#troubleshooting"></a>
</h2>
<ul>
<li>
<strong>Timeouts</strong>: increase <code>n_tries</code> / <code>backoff</code> or supply a custom <code>.post_func</code> with higher <code>timeout()</code>.</li>
<li>
<strong>Model Not Found</strong>: use <code>list_models("&lt;provider&gt;")</code> or consult provider docs.</li>
<li>
<strong>Auth Errors</strong>: verify your API key/token and environment variables.</li>
<li>
<strong>Network Issues</strong>: check VPN/proxy, firewall, or SSL certs.</li>
</ul>
<hr>
</div>
<div class="section level2">
<h2 id="contributing--support">Contributing &amp; Support<a class="anchor" aria-label="anchor" href="#contributing--support"></a>
</h2>
<p>Issues and PRs welcome at <a href="https://github.com/knowusuboaky/chatLLM" class="external-link uri">https://github.com/knowusuboaky/chatLLM</a></p>
<hr>
</div>
<div class="section level2">
<h2 id="license">License<a class="anchor" aria-label="anchor" href="#license"></a>
</h2>
<p>MIT ¬© <a href="mailto:kwadwo.owusuboakye@outlook.com">Kwadwo Daddy Nyame Owusu - Boakye</a></p>
<hr>
</div>
<div class="section level2">
<h2 id="acknowledgements">Acknowledgements<a class="anchor" aria-label="anchor" href="#acknowledgements"></a>
</h2>
<p>Inspired by <strong>RAGFlowChainR</strong>, powered by <strong>httr</strong> and the R community. Enjoy!</p>
</div>
</div>
  </main><aside class="col-md-3"><div class="links">
<h2 data-toc-skip>Links</h2>
<ul class="list-unstyled">
<li><a href="https://cloud.r-project.org/package=chatLLM" class="external-link">View on CRAN</a></li>
<li><a href="https://github.com/knowusuboaky/chatLLM/" class="external-link">Browse source code</a></li>
<li><a href="https://github.com/knowusuboaky/chatLLM/issues" class="external-link">Report a bug</a></li>
</ul>
</div>

<div class="license">
<h2 data-toc-skip>License</h2>
<ul class="list-unstyled">
<li><a href="LICENSE.html">Full license</a></li>
<li><small><a href="https://opensource.org/licenses/mit-license.php" class="external-link">MIT</a> + file <a href="LICENSE-text.html">LICENSE</a></small></li>
</ul>
</div>


<div class="citation">
<h2 data-toc-skip>Citation</h2>
<ul class="list-unstyled">
<li><a href="authors.html#citation">Citing chatLLM</a></li>
</ul>
</div>

<div class="developers">
<h2 data-toc-skip>Developers</h2>
<ul class="list-unstyled">
<li>Kwadwo Daddy Nyame Owusu Boakye <br><small class="roles"> Author, maintainer </small>   </li>
</ul>
</div>

<div class="dev-status">
<h2 data-toc-skip>Dev status</h2>
<ul class="list-unstyled">
<li><a href="https://cran.r-project.org/package=chatLLM" class="external-link"><img src="https://www.r-pkg.org/badges/version/chatLLM" alt="CRAN status"></a></li>
<li><a href="https://app.codecov.io/gh/knowusuboaky/chatLLM?branch=main" class="external-link"><img src="https://codecov.io/gh/knowusuboaky/chatLLM/branch/main/graph/badge.svg" alt="Codecov"></a></li>
<li><a href="https://github.com/knowusuboaky/chatLLM/commits/main" class="external-link"><img src="https://img.shields.io/github/last-commit/knowusuboaky/chatLLM.svg" alt="Last Commit"></a></li>
<li><a href="https://github.com/knowusuboaky/chatLLM/issues" class="external-link"><img src="https://img.shields.io/github/issues/knowusuboaky/chatLLM.svg" alt="Issues"></a></li>
<li><a href="https://cranlogs.r-pkg.org/badges/grand-total/chatLLM" class="external-link"><img src="https://cranlogs.r-pkg.org/badges/grand-total/chatLLM?color=orange" alt="Downloads"></a></li>
</ul>
</div>

  </aside>
</div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Kwadwo Daddy Nyame Owusu Boakye.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.2.</p>
</div>

    </footer>
</div>





  </body>
</html>
